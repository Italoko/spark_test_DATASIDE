{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Desafio: Consumo de Dados para Previsão do Tempo das Cidades do Vale do Paraíba.\n",
        "\n",
        "## Objetivo\n",
        "\n",
        "Avaliar conhecimentos nas linguagens Python e SQL e na engine de processamento Apache Spark.\n",
        "\n",
        "## Descrição\n",
        "\n",
        "Neste desafio, você desenvolverá um notebook que será responsável por extrair dados de previsão do tempo das cidades do Vale do Paraíba, região onde se localiza a Dataside. Para consultar todas as cidades dessa região, utilizaremos a API do IBGE. No caso, basta realizar uma requisição HTTP com o método GET, utilizando a URL abaixo:\n",
        "\n",
        "```\n",
        "https://servicodados.ibge.gov.br/api/v1/localidades/mesorregioes/3513/municipios\n",
        "```\n",
        "\n",
        "Com esses dados, gerar um data frame e a partir dele uma temp view. Ex: \"cities\"\n",
        "\n",
        "Utilizando os nomes das cidades, deverão ser consultados os dados de previsão de tempo para cada cidade. Para realizar essa consulta, poderá ser utilizada qualquer uma das APIs informadas no link abaixo.\n",
        "\n",
        "[Public APIs - Wather](https://github.com/public-apis/public-apis#weather)\n",
        "\n",
        "Obs.: Para algumas, pode ser necessário cadastrar-se para acessar sua API Key. Mas nenhuma delas deve precisar cadastrar cartão de crédito ou adicionar qualquer valor monetário para utilizar. Caso alguma solicite, basta optar por outra.\n",
        "\n",
        "Com os dados consultados, gerar um data frame e partir dele outra temp view. Ex: \"forecasts\"\n",
        "\n",
        "Com as temp views geradas, utilizar Spark SQL para criar queries e gerar data frames das seguintes tabelas:\n",
        "\n",
        "- Tabela 1: dados de previsão do tempo para os próximos cinco dias, para cada data e cidade consultadas. As colunas dessa tabela serão:\n",
        "    - Cidade\n",
        "    - CodigoDaCidade\n",
        "    - Data\n",
        "    - Regiao\n",
        "    - Pais\n",
        "    - Latitude\n",
        "    - Longigute\n",
        "    - TemperaturaMaxima\n",
        "    - TemperaturaMinima\n",
        "    - TemperaturaMedia\n",
        "    - VaiChover\n",
        "    - ChanceDeChuva\n",
        "    - CondicaoDoTempo\n",
        "    - NascerDoSol\n",
        "    - PorDoSol\n",
        "    - VelocidadeMaximaDoVento\n",
        "    \n",
        "    Obs.: Os valores da coluna \"VaiChover\" deverá ser \"Sim\" ou \"Não\". E a coluna \"CodigoDaCidade\" é o ID retornado junto com os nomes da cidades na API do IBGE.\n",
        "    Obs.: Dependendo da API utilizada, algumas colunas podem não existir e ficarão em branco. Você deve optar por uma API que traga o maior número de informações possível.\n",
        "\n",
        "- Tabela 2: quantidade de dias com chuva e sem chuva para os dias consultados, para cada data consultada. Colunas:\n",
        "    - Cidade\n",
        "    - QtdDiasVaiChover\n",
        "    - QtdDiasNaoVaiChover\n",
        "    - TotalDiasMapeados\n",
        "\n",
        "Essas tabelas deverão ser exportadas em formado CSV e entregue no final do desafio.\n",
        "\n",
        "## To Do\n",
        "\n",
        "[ ] - Consultar municípios do Vale do Paraíba, gerar um data frame e criar uma temp view com esses dados.\n",
        "[ ] - Consultar dados do tempo para cada município, gerar um data frame e criar uma outra temp view.\n",
        "[ ] - Utilizar Spark SQL para gerar os data frames das Tabelas 1 e 2.\n",
        "[ ] - Exportar os data frames para CSV.\n",
        "\n",
        "## Atenção\n",
        "\n",
        "- Existe um limite de requisições de 10000 requests por conta cadastrada na m3o.\n",
        "- Essa API pode retornar cidades de outras regiões que possuem nome semelhante a alguma cidade do Vale do Paraiba. Pode mantê-las ou filtrar para gerar as tabelas apenas com dados de Regiao = Sao Paulo. Fica a seu critério.\n",
        "\n",
        "## Entregando o desafio\n",
        "\n",
        "Concluindo todos os passos informados em To Do, basta salvar o arquivo .ipynb do notebook e enviar para a Dataside juntamente com os CSVs das duas tabelas.\n"
      ],
      "metadata": {
        "id": "S02nubKmV_bS"
      },
      "id": "S02nubKmV_bS"
    },
    {
      "cell_type": "code",
      "source": [
        "# Dependências do SPARK\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-2.4.4/spark-2.4.4-bin-hadoop2.7.tgz\n",
        "!tar xf spark-2.4.4-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark"
      ],
      "metadata": {
        "id": "fwCvS4_6Hy4K"
      },
      "id": "fwCvS4_6Hy4K",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importações \n",
        "import os\n",
        "import findspark\n",
        "import requests\n",
        "import json\n",
        "from pyspark.sql import SparkSession"
      ],
      "metadata": {
        "id": "PDqIm2peF7Ol"
      },
      "id": "PDqIm2peF7Ol",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuração das variáveis de ambiente\n",
        "\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.4-bin-hadoop2.7\"\n",
        "\n",
        "# tornar o pyspark \"importável\"\n",
        "findspark.init('spark-2.4.4-bin-hadoop2.7')\n",
        "\n",
        "# Inicia sessão spark\n",
        "spark = SparkSession.builder \\\n",
        "      .master(\"local[1]\") \\\n",
        "      .appName(\"SparkByExamples.com\") \\\n",
        "      .getOrCreate()"
      ],
      "metadata": {
        "id": "u1YYK1quHkCX"
      },
      "id": "u1YYK1quHkCX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extrair dados de previsão do tempo das cidades do Vale do Paraíba, região onde se localiza a Dataside. Para consultar todas as cidades dessa região, utilizaremos a API do IBGE. No caso, basta realizar uma requisição HTTP com o método GET, utilizando a URL abaixo:\n",
        "\n",
        "```\n",
        "https://servicodados.ibge.gov.br/api/v1/localidades/mesorregioes/3513/municipios\n",
        "```\n",
        "Com esses dados, gerar um data frame e a partir dele uma temp view. Ex: \"cities\""
      ],
      "metadata": {
        "id": "GoXjTHZUOvWI"
      },
      "id": "GoXjTHZUOvWI"
    },
    {
      "cell_type": "code",
      "source": [
        "def get_cities(url):\n",
        "    \"\"\"\n",
        "        Recebe como parâmetro a URL da API IBGE\n",
        "        Retorna uma lista de cidades e algumas informações sobre elas, como: \n",
        "        - Código da cidade usada na API IBGE\n",
        "        - Nome da cidade\n",
        "        - Nome do estado \n",
        "        - Sigla do estado\n",
        "        - Nome da região \n",
        "        - Sigla da Região\n",
        "    \"\"\"\n",
        "    response_city = requests.get(url)\n",
        "    response_city = response_city.json()\n",
        "    \n",
        "    cities = list()\n",
        "\n",
        "    for i in response_city: \n",
        "        cities.append(\n",
        "            dict(\n",
        "                cidade_id = i.get('id')\n",
        "                , cidade_nome = i.get('nome') \n",
        "                , estado_nome = i.get('microrregiao').get('mesorregiao').get('UF').get('nome')\n",
        "                , estado_sigla = i.get('microrregiao').get('mesorregiao').get('UF').get('sigla')\n",
        "                , regiao_nome = i.get('microrregiao').get('mesorregiao').get('UF').get('regiao').get('nome')\n",
        "                , regiao_sigla = i.get('microrregiao').get('mesorregiao').get('UF').get('regiao').get('sigla')\n",
        "            )\n",
        "        )\n",
        "\n",
        "    return cities"
      ],
      "metadata": {
        "id": "iUfqcG3xu_8O"
      },
      "id": "iUfqcG3xu_8O",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cdc739f4-2ebf-4ff2-91ff-89689510e618",
      "metadata": {
        "tags": [],
        "id": "cdc739f4-2ebf-4ff2-91ff-89689510e618"
      },
      "outputs": [],
      "source": [
        "# Buscar cidades do Vale do Paraíba\n",
        "resp_cities = get_cities('https://servicodados.ibge.gov.br/api/v1/localidades/mesorregioes/3513/municipios')\n",
        "\n",
        "# Criar data frame com as cidades\n",
        "df_cidades = spark.createDataFrame(resp_cities)\n",
        "\n",
        "# Criar view com as cidades\n",
        "df_cidades.createOrReplaceTempView(\"cities\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#API_KEY_ACCS\n",
        "op1 = 'sDDNoolJDvW2ftkQQ67WhtvwlWVHJaph'\n",
        "op2 = 'tWjNMi1PjXvh4A5ZLy8kSYEoSBNtvmWg'"
      ],
      "metadata": {
        "id": "GnJyxb6pKAt1"
      },
      "id": "GnJyxb6pKAt1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Configuração parâmetros API Wather\n",
        "api_wather_settings = {\n",
        "    'url_base': 'http://dataservice.accuweather.com/'\n",
        "    , 'api_key_value': op1\n",
        "}"
      ],
      "metadata": {
        "id": "Q9mfjkSDMy6_"
      },
      "id": "Q9mfjkSDMy6_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_location_info(city):\n",
        "    \"\"\"\n",
        "        Recebe como parâmetro o nome da cidade\n",
        "        Retorna informações sobre a localidade, como: \n",
        "        - location_key (chave usada na busca da API)\n",
        "        - País\n",
        "        - Latitude\n",
        "        - Longitude\n",
        "    \"\"\"\n",
        "    url_loc_key = api_wather_settings.get('url_base','') + 'locations/v1/cities/search?apikey='\\\n",
        "            + api_wather_settings.get('api_key_value','') + '&q=' + city + '&language=pt-br'\n",
        "\n",
        "    response_loc_key = requests.get(url_loc_key)\n",
        "    response_loc_key = response_loc_key.json()\n",
        "    response_loc_key = response_loc_key[0]\n",
        "\n",
        "    location_info = dict(\n",
        "        key = response_loc_key.get('Key')\n",
        "        , country = response_loc_key.get('Country').get('LocalizedName')\n",
        "        , lat = response_loc_key.get('GeoPosition').get('Latitude')\n",
        "        , lon = response_loc_key.get('GeoPosition').get('Longitude')\n",
        "    )\n",
        "\n",
        "    return location_info"
      ],
      "metadata": {
        "id": "85IoT60e6UnE"
      },
      "id": "85IoT60e6UnE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Utilizando os nomes das cidades, deverão ser consultados os dados de previsão de tempo para cada cidade. Para realizar essa consulta, poderá ser utilizada qualquer uma das APIs informadas no link abaixo.\n",
        "\n",
        "[Public APIs - Wather](https://github.com/public-apis/public-apis#weather)\n",
        "\n",
        "Com os dados consultados, gerar um data frame e partir dele outra temp view. Ex: \"forecasts\""
      ],
      "metadata": {
        "id": "WyfURu-qPa7z"
      },
      "id": "WyfURu-qPa7z"
    },
    {
      "cell_type": "code",
      "source": [
        "def get_forecasts(column, table, limit=''):\n",
        "    \"\"\"\n",
        "        Recebe como parâmetro \n",
        "            - Coluna a ser consultada*\n",
        "            - Tabela a ser consultada*\n",
        "            - Limite de cidades a serem consultadas (Opcional) \n",
        "\n",
        "        Retorna previsão do tempo de 5 dias para cada cidade consultada\n",
        "    \"\"\"\n",
        "    cities = spark.sql(f'select {column} from {table} limit {limit}').rdd.flatMap(lambda x: x).collect()\n",
        "\n",
        "    forecasts = list()\n",
        "\n",
        "    for city in cities:\n",
        "        \"\"\"\n",
        "            Como a API escolhida tem limite de requisição diária\n",
        "            Aproveitamos a requisição para obter location_key para obter dados geográficos\n",
        "        \"\"\"\n",
        "        # Busca informações geográfica da cidade\n",
        "        location_info = get_location_info(city)\n",
        "        \n",
        "        # Monta url para buscar forecast\n",
        "        location_key = location_info.get('key','') \n",
        "        url_forecast = api_wather_settings.get('url_base','') + 'forecasts/v1/daily/5day/'+ location_key\\\n",
        "            + '?apikey=' + api_wather_settings.get('api_key_value','') + '&language=pt-br&details=true'\n",
        "\n",
        "        response_forecast = requests.get(url_forecast)\n",
        "        response_forecast = response_forecast.json()\n",
        "        response_forecast = response_forecast['DailyForecasts']\n",
        "\n",
        "        for day in response_forecast: \n",
        "            # Monta lista de forecasts para cada cidade\n",
        "            forecasts.append(\n",
        "                dict(\n",
        "                     cidade_nome = city\n",
        "                     , pais = location_info.get('country')\n",
        "                     , latitude = location_info.get('lat')\n",
        "                     , longitude = location_info.get('lon')\n",
        "                     , data = day.get('Date')\n",
        "                     , temperatura_minima = day.get('Temperature').get('Minimum').get('Value') \n",
        "                     , temperatura_maxima = day.get('Temperature').get('Maximum').get('Value') \n",
        "                     , temperatura_media = (day.get('Temperature').get('Maximum').get('Value')\\\n",
        "                                            + day.get('Temperature').get('Minimum').get('Value')) / 2\n",
        "                     , vai_chover = day.get('Day').get('HasPrecipitation')\n",
        "                     , chance_de_chuva = day.get('Day').get('RainProbability')\n",
        "                     , condicao_do_tempo = day.get('Day').get('ShortPhrase')\n",
        "                     , nascer_do_sol = day.get('Sun').get('Rise')\n",
        "                     , por_do_sol = day.get('Sun').get('Set')\n",
        "                     , velocidade_max_do_vento = day.get('Day').get('Wind').get('Speed').get('Value')\n",
        "                )\n",
        "            )\n",
        "\n",
        "    return forecasts"
      ],
      "metadata": {
        "id": "qj0_OglCOISb"
      },
      "id": "qj0_OglCOISb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c4a40a6f-d5f1-4524-9d0b-d1e6e24dfbfa",
      "metadata": {
        "tags": [],
        "id": "c4a40a6f-d5f1-4524-9d0b-d1e6e24dfbfa"
      },
      "outputs": [],
      "source": [
        "# Buscar previsão do tempo para as cidades\n",
        "resp_forecasts = get_forecasts(column = 'cidade_nome', table = 'cities')\n",
        "\n",
        "# Criar data frame com as previsões\n",
        "df_forecasts = spark.createDataFrame(resp_forecasts)\n",
        "\n",
        "# Criar view com as previsões\n",
        "df_forecasts.createOrReplaceTempView(\"forecasts\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Criar DF da Tabela 1\n",
        "tab1_sql = \"\"\"\n",
        "    select \n",
        "        c.cidade_nome as Cidade\n",
        "        , c.cidade_id as CodigoDaCidade\n",
        "        , substr(f.data, 0, 10) as Data\n",
        "        , c.regiao_nome as Regiao\n",
        "        , f.pais as Pais \n",
        "        , f.latitude as Latitude\n",
        "        , f.longitude as Longitude\n",
        "        , round((f.temperatura_maxima -32.0) / 1.8, 2) || ' Cº' as TemperaturaMaxima\n",
        "        , round((f.temperatura_minima -32.0) / 1.8, 2) || ' Cº' as TemperaturaMinima\n",
        "        , round((f.temperatura_media -32.0) / 1.8, 2) || ' Cº' as TemperaturaMedia\n",
        "        , case when f.vai_chover then 'Sim' else 'Não' end as VaiChover\n",
        "        , f.chance_de_chuva as ChanceDeChuva\n",
        "        , f.condicao_do_tempo as CondicaoDoTempo\n",
        "        , substr(f.nascer_do_sol, 12, 8) as NascerDoSol\n",
        "        , substr(f.por_do_sol, 12, 8) as PorDoSol\n",
        "        , round(f.velocidade_max_do_vento * 1.61, 2) || ' km/h' as VelocidadeMaximaDoVento \n",
        "\n",
        "    from forecasts f\n",
        "        inner join \n",
        "    cities c\n",
        "        on \n",
        "    f.cidade_nome = c.cidade_nome\n",
        "\"\"\"\n",
        "\n",
        "df_1 = spark.sql(tab1_sql)"
      ],
      "metadata": {
        "id": "kC_bS2jxr-i6"
      },
      "id": "kC_bS2jxr-i6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Criar DF da Tabela 2\n",
        "tab2_sql = \"\"\"\n",
        "    select \n",
        "        vai_chover.cidade_nome as Cidade\n",
        "        , vai_chover.QtdDiasVaiChover\n",
        "        , n_vai_chover.QtdDiasNaoVaiChover\n",
        "        , vai_chover.QtdDiasVaiChover + n_vai_chover.QtdDiasNaoVaiChover as TotalDiasMapeados\n",
        "    from \n",
        "    (\n",
        "        select \n",
        "            cidade_nome\n",
        "            , count(vai_chover) as QtdDiasVaiChover\n",
        "        from forecasts \n",
        "        where vai_chover = true\n",
        "        group by cidade_nome\n",
        "    ) vai_chover\n",
        "    \n",
        "    inner join \n",
        "    \n",
        "    (\n",
        "        select \n",
        "            cidade_nome\n",
        "            , count(vai_chover) as QtdDiasNaoVaiChover\n",
        "        from forecasts \n",
        "        where vai_chover = false\n",
        "        group by cidade_nome\n",
        "    ) n_vai_chover\n",
        "\n",
        "    on vai_chover.cidade_nome = n_vai_chover.cidade_nome\n",
        "\n",
        "\"\"\"\n",
        "df_2 = spark.sql(tab2_sql)"
      ],
      "metadata": {
        "id": "Sx0INiti7131"
      },
      "id": "Sx0INiti7131",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1ff378b-4c24-47dc-aba1-742211cd385d",
      "metadata": {
        "id": "c1ff378b-4c24-47dc-aba1-742211cd385d"
      },
      "outputs": [],
      "source": [
        "# Exportar CSVs\n",
        "df_1.repartition(1).write.option(\"header\",True).\\\n",
        "    mode(\"overwrite\").csv(\"tabela1.csv\")\n",
        "\n",
        "df_2.repartition(1).write.option(\"header\",True).\\\n",
        "    mode(\"overwrite\").csv(\"tabela2.csv\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "name": "spark_test-DATASIDE.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}